# Cuber - 高度な知識グラフシステム

Cuberは、非構造化文書をクエリ可能な相互接続された知識に変換する高性能知識グラフシステムです。

## Cuberが優れている理由

### 統合アーキテクチャ
従来のシステムではベクトルストレージとグラフデータに別々のデータベースが必要でしたが、Cuberは統合されたDBを使用し、統合の複雑さを排除してパフォーマンスを向上させています。

### インテリジェントな処理パイプライン
Cuberは単にテキストを保存するだけでなく、理解します。システムは文書からエンティティ、関係、文脈情報を自動的に抽出し、セマンティック類似性のベクトル検索と関係認識推論のグラフトラバーサルを組み合わせます。データサイズに基づいてバルク処理とバッチ処理を賢く切り替え、エッジデバイス実行を広くカバーするようメモリ使用量とスループットを最適化します。

### 自己成長する知識ベース - 人間のような学習プロセス

Cuberの最も革新的な特徴は、自己改善能力です。Memify機能を通じて、システムは人間のように自身の知識を振り返り、成長します：

**自己認識と謙虚さ**  
Cuberは「知らないこと」「できないこと」を明確に認識します。質問に答えられなかった事柄をUnknown（未知）として記録し、これを恥じるのではなく、成長の機会として捉えます。

**自問自答による成長**  
Unknownに直面すると、Cuberは自ら問いを立て、既存の知識を深く探索します。「なぜわからないのか？」「関連する知識は何か？」「どのように考えれば解決できるか？」という自問自答のプロセスを経て、新しい洞察に到達しようと試みます。

**成長の記録と感謝**  
解決に成功した場合、Cuberは新しく獲得した能力（Capability）として記録します。ただし、単に「できるようになった」だけでなく、以下を明確に記憶します：
- いつこの能力を獲得したのか（タイムスタンプ）
- どの情報源から学んだのか（文書への参照）
- どのような推論プロセスで到達したのか（思考の記録）
- 誰の問いかけがきっかけだったのか（ユーザーへの帰属）

この謙虚で誠実な学習プロセスは、人間の成長過程を模倣しています。

**知識の統合と洗練**  
さらに、Cuberは断片的な知識を統合します。類似する洞察を発見すると、それらを結晶化（Crystallization）によって一つの高次な理解へと昇華させます。冗長性を排除しながら本質を保持し、使わない知識は自然に忘却されます。これは人間の記憶の仕組みと同じです。

## インストール

```bash
go get mycute/pkg/cuber
```

### 依存関係
- Go 1.25+
- OpenAI互換のLLM API（embeddings と completion用）

## クイックスタート

### 基本的な使い方

```go
package main

import (
    "context"
    "log"
    "github.com/t-kawata/mycute/pkg/cuber"
    "github.com/t-kawata/mycute/pkg/cuber/tools/query"
)

func main() {
    // 1. サービスの設定
    config := cuber.CuberConfig{
        CompletionAPIKey:   "sk-...",  // OpenAI APIキー
        EmbeddingsAPIKey:   "sk-...",  // OpenAI APIキー
        S3UseLocal:         true,       // ローカルストレージを使用
        S3LocalDir:         "data/files", // 保存先
        S3DLDir:            "data/cache", // ダウンロードキャッシュ
    }

    // 2. サービスインスタンスの作成
    svc, err := cuber.NewCuberService(config)
    if err != nil {
        log.Fatal(err)
    }
    defer svc.Close()

    ctx := context.Background()
    cubePath := "data/db/knowledge.db"
    group := "user1-dataset1"

    // 3. 文書の吸収（取り込み + 知識グラフ構築）
    files := []string{"document.txt", "paper.txt"}
    if _, err := svc.Absorb(ctx, cubePath, group, files); err != nil {
        log.Fatal(err)
    }

    // 4. 知識ベースの検索
    result, _, err := svc.Query(ctx, 
        cubePath,
        group,
        query.QueryTypeGraphCompletion,
        "主な概念は何ですか？",
    )
    if err != nil {
        log.Fatal(err)
    }

    log.Println("回答:", result)
}
```

## コア機能

### `Absorb` - 統合取り込みと処理

**目的**: 文書から知識への1ステップ変換

```go
_, err := svc.Absorb(ctx, cubeDbFilePath, memoryGroup, filePaths)
```

**内部処理の流れ**:

1. **ファイル取り込み (`add`)**
   - ファイルをS3/ローカルストレージにアップロード
   - メタデータをDBの`data`テーブルに保存
   - 重複排除のためSHA-256ハッシュを計算
   - グループベースのパーティショニング（`user-dataset`）

2. **知識グラフ構築 (`cognify`)**
   - **チャンク化**: 文書を1024トークンのチャンクに分割（20トークンのオーバーラップ）
   - **グラフ抽出**: エンティティと関係を抽出
   - **ストレージ**: チャンク、ノード、エッジをDBに永続化
   - **要約**: 各チャンクを要約し、高速検索のために埋め込みベクトル化
   - **クリーンアップ**: 処理後、元ファイルを自動削除

**結果**: セマンティック検索可能な構造化知識グラフ

---

### `Query` - インテリジェントなクエリ解決

**目的**: ハイブリッドベクトル+グラフ検索を使用して質問に回答

```go
result, _, err := svc.Query(ctx, cubeDbFilePath, memoryGroup, queryType, text)
```

**検索タイプ**:

#### `SUMMARIES` - 要約のみの高速検索
**用途**: 大まかな情報を素早く取得したい場合  
**処理**: 文書要約のベクトル検索のみを実行  
**返り値の例**:
```
「この文書では、DBの統合アーキテクチャについて説明しています。
主なポイントは、ベクトルストレージとグラフストレージの統合により、
パフォーマンスが向上し、複雑さが削減されることです。」
```

#### `GRAPH_COMPLETION` - 完全な文脈理解検索（推奨）
**用途**: 正確で詳細な回答が必要な場合  
**処理**: ベクトル検索でチャンクを取得 → グラフでエンティティと関係を拡張 → LLMが統合して回答  
**返り値の例**:
```
「DBの主な特徴は以下の3つです：

1. ベクトル検索機能
   DBは、ベクトルの類似度検索が可能で、コサイン類似度を使用した
   検索に対応しています。これにより、セマンティックな類似性に基づいた
   情報検索が実現します。

2. グラフストレージ
   ノードとエッジの管理機能を持ち、Cypherクエリ言語をサポートしています。
   エンティティ間の複雑な関係を効率的に保存・検索できます。

3. 統合アーキテクチャ
   従来のベクトルとグラフの機能を単一のシステムに統合しており、
   内部の高速通信を通じて高度で高速なレスポンスを実現します。

[参照: document.txt, chunk_id: abc123, entity: DB]」
```

#### `GRAPH_SUMMARY_COMPLETION` - ハイブリッドアプローチ
**用途**: 要約と詳細のバランスを取りたい場合  
**処理**: グラフ検索で関連エンティティを特定 → 要約を取得 → LLMが補完  
**返り値の例**:
```
「データベースの統合について：

DBは、ベクトル検索とグラフストレージを統合した高性能データベースです。
この統合により、別々のデータベースを管理する複雑さが排除され、
クエリパフォーマンスが大幅に向上しています。

関連する他の概念として、従来のベクトルストレージおよび
グラフストレージがありますが、これらは現在DBに統合されています。

[要約ソース: summary_id: xyz789]」
```

**内部処理の流れ**:

1. **クエリの埋め込み**
   - クエリをベクトルに変換

2. **ベクトル検索**
   - コサイン類似度でTop-K類似チャンクを取得
   - チャンクから関連エンティティ名を特定

3. **グラフ拡張**
   - 知識グラフでエンティティを検索
   - 接続されたエンティティと関係を取得
   - グラフトラバーサル（1〜? ホップ）でコンテキストを拡張

4. **回答生成**
   - LLMが以下から回答を合成:
     - 関連テキストチャンク
     - エンティティ定義
     - 関係のコンテキスト
   - 引用を含めて回答

---

### `Memify` - 人間のような自己成長プロセス

**目的**: 知識ベースの自己認識、自己反省、自己改善

```go
config := &cuber.MemifyConfig{
    RecursiveDepth:     1,     // 再帰的成長の深さ（後述）
    PrioritizeUnknowns: true,  // 知識ギャップを優先的に解決
}
_, err := svc.Memify(ctx, cubeDbFilePath, memoryGroup, config)
```

#### RecursiveDepth - 再帰的成長の深さについて

`RecursiveDepth`は、Cuberが自己成長のサイクルを何回繰り返すかを指定します。

**値の意味**:
- `0`: 1回のみの成長サイクル
  - Unknown解決（フェーズA）を実行
  - 知識統合（フェーズB）を1回実行
  - 多くの実用ケースではこれで十分

- `1`: 2回の成長サイクル
  - 1回目の成長で得られた新しい知識を基に、さらに成長
  - 例: 1回目で「AとBの関係」を学習 → 2回目で「AとBの関係からCが導かれる」という高次の洞察を獲得
  - より深い理解と抽象化が可能

- `2以上`: 複数回の成長サイクル
  - 各サイクルで知識が階層的に構造化・抽象化される
  - 計算コストが高いため、特別な用途にのみ推奨
  - 例: 複数の論文から段階的に理論を構築する研究用途

**推奨設定**:
- 日常的な使用: `RecursiveDepth: 0`
- 深い理解が必要: `RecursiveDepth: 1`
- 理論構築・研究: `RecursiveDepth: 2以上`

各サイクルで知識は洗練されますが、トークン消費とAPIコストが指数的に増加する点に注意してください。

#### 内部処理の流れ - 人間のような成長

**フェーズA: 自己認識と謙虚な学習** (`PrioritizeUnknowns = true`の場合)

Cuberは、人間が「わからない」と認めることから学びが始まるように、まず自身の無知を認識します。

1. **無知の発見**
   - 過去に答えられなかった質問（Unknown）を洗い出す
   - 「この質問には答えられなかった」という事実を謙虚に受け止める

2. **自問自答**
   - 各Unknownに対して、LLMが自ら問いを生成
   - 「なぜこれがわからないのか？」
   - 「どんな前提知識が必要か？」
   - 「既存の知識のどこに手がかりがあるか？」

3. **既存知識の深い探索**
   - ベクトル検索で関連する可能性のある知識を幅広く探す
   - グラフトラバーサルで間接的な関連性も調査
   - 「この情報とあの情報を組み合わせたら答えが出るかもしれない」という試行錯誤

4. **洞察の獲得と記録**
   - 推論に成功した場合、新しいCapability（能力）として登録
   - 記録内容:
     ```
     能力: 「XとYの関係からZが導かれることを理解できる」
     獲得日時: 2024-12-08 09:30:15
     情報源: ["document_A.txt", "paper_B.pdf"]
     推論経路: ["XはYに影響する" → "YはZを決定する" → "ゆえにXはZに影響する"]
     きっかけ: user1の質問「XはZに影響しますか？」がきっかけで解決
     ```
   - Unknownを「解決済み」としてマーク
   - 解決に使用した知識への感謝をエッジとして記録

このプロセスは人間が「先生に質問 → 自分で調べる → 理解する → ノートに記録する」という学習過程と同じ構造を持ちます。

**フェーズB: 知識の統合と洗練**

1. **パターンの発見（ルール抽出）**
   - 文書チャンクから一般化可能なパターンを抽出
   - 「多くの事例に共通する法則は何か？」を探る
   - 発見したルールをノードとして保存

2. **自己反省 (Self-Reflection)**
   - 各チャンクに対して「このチャンクから何が学べるか？」と問いかける
   - 関連するルールや既存知識と照らし合わせる
   - メタ洞察（知識についての知識）を生成
   - 反省内容をエッジとして記録

3. **知識の結晶化 (Crystallization)**
   - 類似するルールを発見（ベクトル類似度 > 0.8）
   - 「これらは本質的に同じことを言っているのでは？」と気づく
   - LLMが複数のルールを一つの洗練された洞察に統合
   - 冗長な古いルールは削除し、結晶化された知識のみ保持
   - 例:
     ```
     ルール1: 「変更があったら必ずテストする」
     ルール2: 「コード修正後はテストを実行すべき」
     ルール3: 「テストなしでリリースしない」
     ↓ 結晶化
     統合ルール: 「コード変更時は、リリース前に必ずテストを実行し、品質を保証する」
     ```

4. **忘却と整理 (Pruning)**
   - 長期間使われていない知識（孤立ノード）を特定
   - 猶予期間を設けて本当に不要か確認
   - 不要と判断された知識は削除
   - 人間の記憶と同様、使わない知識は自然に忘れる

**適応的処理**:
- **バルクモード**: <50K文字を単一のLLM呼び出しで処理（小規模データセットに最適）
- **バッチモード**: >50K文字を20%のオーバーラップで分割（大規模データセットに対してメモリ効率的）

**結果**: 
人間のように成長する知識ベース。無知を認め、質問し、学び、記録し、統合し、忘却する。このサイクルを繰り返すことで、より賢く、より洗練された知識システムへと進化します。

---

## 設定

### 必須設定

```go
type CuberConfig struct {
    // データベース
    DBDirPath string // 例: "data/db"
    
    // LLM - Completion
    CompletionAPIKey  string // 必須
    CompletionBaseURL string // オプション（プロキシ用）
    CompletionModel   string // オプション（デフォルト: gpt-4o）
    
    // LLM - Embeddings
    EmbeddingsAPIKey  string // 必須
    EmbeddingsBaseURL string // オプション
    EmbeddingsModel   string // オプション（デフォルト: text-embedding-3-small）
    
    // ストレージ
    S3UseLocal bool   // true = ローカルファイル, false = AWS S3
    S3LocalDir string // ローカル保存先ディレクトリ
    S3DLDir    string // ダウンロードキャッシュディレクトリ
}
```

### 高度な設定

```go
// Memify設定
MemifyMaxCharsForBulkProcess int // デフォルト: 50000
MemifyBatchOverlapPercent    int // デフォルト: 20
MemifyBatchMinChars          int // デフォルト: 5000

// メタ認知閾値
MetaSimilarityThresholdUnknown         float64 // デフォルト: 0.3
MetaSimilarityThresholdReflection      float64 // デフォルト: 0.5
MetaSimilarityThresholdCrystallization float64 // デフォルト: 0.8

// グラフ代謝
GraphMetabolismAlpha           float64 // 強化率, デフォルト: 0.2
GraphMetabolismDelta           float64 // 減衰率, デフォルト: 0.3
GraphMetabolismPruneThreshold  float64 // 剪定閾値, デフォルト: 0.1
GraphPruningGracePeriodMinutes int     // デフォルト: 60
```

## 使用例ワークフロー

### 文書Q&Aシステム

```go
// 知識ベースの取り込み
svc.Absorb(ctx, cubePath, "bot-support", []string{"manual.pdf", "faq.md"})

// ユーザーの質問に回答
answer, _, _ := svc.Query(ctx, cubePath, "bot-support", 
    query.QueryTypeGraphCompletion, "パスワードをリセットする方法は？")
```

### 研究アシスタント

```go
// 研究論文をロード
svc.Absorb(ctx, cubePath, "researcher-research_2024", paperFiles)

// メタ認知で強化
svc.Memify(ctx, cubePath, "researcher-research_2024", &cuber.MemifyConfig{
    RecursiveDepth: 1,
})

// 引用付きでクエリ
svc.Query(ctx, cubePath, "researcher-research_2024", 
    query.QueryTypeGraphCompletion, "Xに関する最新の知見は？")
```

### 段階的な知識構築

```go
// 1日目: 初期知識
svc.Absorb(ctx, cubePath, "team-project_docs", week1Files)

// 7日目: 同じデータセットにさらに文書を追加
svc.Absorb(ctx, cubePath, "team-project_docs", week2Files)

// 知識を統合
svc.Memify(ctx, cubePath, "team-project_docs", nil)
```

## 従来のRAGに対する主な利点

| 機能 | 従来のRAG | Cuber |
|---------|----------------|-------|
| **ストレージ** | 別々のベクトルDB + グラフDB | 統合DB |
| **検索** | シンプル | ハイブリッドベクトル + グラフ |
| **コンテキスト** | 固定チャンクコンテキスト | 動的グラフ拡張 |
| **更新** | 手動再インデックス | Memifyによる自動 |
| **推論** | なし | メタ認知 + エンティティ関係 |
| **自己改善** | なし | Unknown認識と解決 |
| **知識統合** | なし | 結晶化による洗練 |

## 適用例

### ドメイン知識システム
- 企業の内部文書を知識ベース化
- 従業員からの質問に自動回答
- 定期的なMemifyで知識を最新化・洗練

### 技術ドキュメント検索
- APIドキュメント、マニュアルを取り込み
- 開発者の質問に文脈を理解して回答
- コード例と説明を関連付けて提示
- わからなかった質問から自己学習

### 研究支援システム
- 論文、書籍を自動解析
- 研究テーマに関する洞察を抽出
- 関連研究の発見と要約
- 知識の矛盾や欠落を自己認識し、再帰的に深化
