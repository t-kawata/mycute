# フェーズ 27: 時間的知識グラフ（Temporal Knowledge Graph）による概念ドリフトの解決と知識の生命化

このディレクティブは、Cuberの知識グラフにおける時間軸（Unixタイムスタンプ）の導入と、それが「Absorb」「Memify」「Query」の各プロセスにおいてどのように作用し、「概念のドリフト（Concept Drift）」という致命的な課題をいかに解決するかについて、その全貌を詳説します。

---

## 1. 根本課題：静的な知識と概念のドリフト（Concept Drift）

知識とは、時間の経過とともに刻一刻と変化する生き物です。ある時点では正論であったことが、新しい情報の出現によって修正されたり、あるいは完全に逆転したりすることがあります。これが「概念のドリフト（Concept Drift）」です。

従来のCuberの実装では、知識（エッジ）に対して「強さ（Weight）」と「正しさ（Confidence）」を付与していましたが、そこに「いつ（Unix）」という時間軸が欠落していました。時間軸がないシステムは、過去の誤った確信と、現在の新しい発見を同列に扱い、結果として情報の矛盾に沈黙するか、古い情報に引きずられて誤った回答を導き出すことになります。

本フェーズでは、`GraphEdge` に **秒単位のUnixタイムスタンプ（`unix`）** を導入することで、知識のライフサイクル全体を一貫した時間的枠組みの中で管理します。

---

## 2. 三大指標による知識のモデリング

Cuber ネットワークを流れるすべての「つながり」は、以下の 3 つの動的な指標によって定義され、これらが相互に作用することで「成長・変化・進化」が表現されます（GraphEdgeテーブル）。

| 指標 | 意味 | 役割 |
| :--- | :--- | :--- |
| **Weight** | つながりの強さ | その関係がどれだけ「太く」「重要」であるか。 |
| **Confidence** | つながりの正確さ | その関係がどれだけ「正しい」か、情報の「純度」を表現。 |
| **Unix** | 観測・更新の時刻 | その関係がどれだけ「鮮烈（フレッシュ）」であるか。 |

これら 3 つを統合した値は、実質的にそのエッジ（つながり）の **「太さ」** を表現しています。太いエッジはシステムにとって重要な知識であり、細いエッジは補助的、あるいは信頼性の低い知識となります。

---

## 3. 「剪定（Pruning）」と「忘却（Forgetting）」：知識の健康維持

エッジの「太さ」を定義することは、単に検索順位を決めるだけでなく、システムにおける **「忘却」** のメカニズムを実装する上で決定的な意味を持ちます。

### 知識の代謝としての忘却
人間の脳が不要なシナプスを削ぎ落とす（剪定）のと同様に、Cuber もまた無限に増え続ける情報の中から、価値の低い情報を能動的・受動的に削ぎ落とす必要があります。

1.  **受動的な忘却（時間減衰）**:
    `unix` が古くなるにつれ、そのエッジの「時間的な太さ」は自動的に減少していきます。長期間更新も再観測もされない知識は、時間の経過とともに自然と「細く」なり、やがてシステムの主要な推論パスから消えていきます。
2.  **能動的な忘却（矛盾と修正）**:
    新しい事実（`confidence: 1.0`）によって、過去の知識の `confidence` が極端に低下した場合、そのエッジは致命的に「細く」なります。
3.  **物理的な剪定（Pruning）**:
    「太さ」が一定の閾値を下回ったエッジを物理的に削除（Delete）することで、知識グラフの肥大化を防ぎ、計算リソースを最適化することができます。

### 記述長最小化原理（MDL原理）によるノードの物理削除と本質的抽出

エッジの剪定（Pruning）が行われた結果、知識グラフ上には接続を失った孤立ノードや、情報の価値が相対的に低下したノードが残留することになります。これらに対して、Cuberは単に「接続がないから消す」という単純なルールではなく、**記述長最小化原理（MDL: Minimum Description Length Principle）** という、より深遠で合理的な数学的指標を用いて、そのノードが「知性にとって残すべき価値があるか」を厳密に評価します。

#### 1. MDLの基本思想：「最小の記述」が「最高の本質」を捉える
MDLは、14世紀の哲学者オッカムが提唱した「オッカムの剃刀（ある事象を説明するのに、必要以上に多くの仮定を設けるべきではない）」という概念を、現代の情報理論に基づき数学的に定式化したものです。

一言で言えば、**「対象となるデータ（知識グラフ）を、最も短いビット数で表現できるモデルこそが、そのデータの本質を最も正確に捉えた最適なモデルである」**という考え方です。知識グラフを圧縮しようとする圧力は、そのまま「ノイズを排し、本質のみを抽出する」プロセスへと変換されます。

#### 2. コスト関数：記述長のトレードオフ
知識グラフ $G$ の総記述長 $L(G)$ は、以下の2つの要素の和として定義されます。この合算値を最小化することが、Cuberの「忘却」の最終的なゴールです。

$$L(G) = \mathbf{L(M)} + \mathbf{L(D|M)}$$

*   **$L(M)$：モデル記述長 (Model Description Length)**
    *   **意味**: 知識グラフの「骨組み（構造自体）」を記述するために必要な情報量です。
    *   **具体例**: グラフに含まれるノードの総数、エッジの総数、およびそれらの型定義などがこれにあたります。
    *   **性質**: ノードを削除したり、似た概念を一つに統合（Supernode化）してグラフをシンプルにするほど、この値は**減少**します。
*   **$L(D|M)$：データ記述長 (Data Description Length)**
    *   **意味**: そのシンプルになったモデル（骨組み）を使って、元の詳細なデータを再構築するために必要な「補正情報（誤差）」の量です。
    *   **具体例**: 削除されたノードが持っていた固有のプロパティや、統合によって失われたディテールの差異。
    *   **性質**: グラフを単純にしすぎると、失われた情報を補完するために膨大な追加説明が必要になるため、この値は**急増**します。

この2指標のバランスにより、**「グラフを簡略化してスッキリさせるメリット（$L(M)$ の減少）」が「情報を失うことによるデメリット（$L(D|M)$ の増大）」を上回る場合に限り、ノードは物理的に削除されるべきである**という、極めて合理的な判断が可能になります。

#### 3. Cuberにおける局所的近似（Local MDL）の実装
理論上の厳密なMDL計算は、数百万ノードのグラフを扱う現代のシステムでは不可能な計算コスト（$O(N^2)$以上）を要します。そのため、本フェーズの実装では、Go言語とLadybugDB（Kuzu）の特性を活かした**「局所MDL（Local MDL）」と「ベクトル類似度」**による高度な近似アプローチを採用します。

判定は、以下の簡略化された判定式に基づき、ノードごとに独立した並列処理として実行されます。

$$ \text{判定スコア} = \underbrace{\text{ストレージ削減量（固定）}}_{L(M)\text{の減少}} - \underbrace{\text{情報の復元困難度（可変）}}_{L(D|M)\text{の増加}} $$

*   **情報の復元困難度の算出**:
    ここがMDLの肝ですが、複雑な確率計算の代わりに**埋め込みベクトルの類似度**を使用します。
    1.  対象ノードのベクトル $V_{target}$ と、その近傍ノード群のベクトル平均 $V_{neighbors}$ を取得します。
    2.  両者のコサイン類似度が高い場合、「このノードの情報は、隣接する他の知識から容易に推測可能（冗長）」であるとみなし、復元困難度を低く（削除しやすく）見積もります。
    3.  逆に、孤立していても周囲と全く異なるベクトル（特異点）を持つ場合、「このノードを消すと、このユニークな情報は二度と復元できない」と判断し、復元困難度を極めて高く（残すべき知識として）設定します。

*   **なぜベクトル類似度でMDLを近似できるのか：「意味の近さ」は「情報の冗長性」を意味する**:
    MDLの核心にある「復元困難度 $L(D|M)$」は、本来「ある情報を失った場合、それを他の情報から再構築するためにどれだけの追加情報（ビット）が必要か」という確率的な量です。これをベクトル類似度で代替できるのは、以下の理論的な橋渡しによります。

    現代の言語モデルが生成する埋め込みベクトルは、単語や概念の**「意味的な位置」**を高次元空間にマッピングしたものです。
    *   **ベクトルが近い（類似度が高い）２つの概念**は、意味的に「言い換え可能」「相互に推測可能」であることを意味します。
        *   例：「東京」と「日本の首都」のベクトルは近い。片方を消しても、もう片方があれば情報をほぼ完全に復元できます。
    *   **ベクトルが遠い（類似度が低い）概念**は、他の情報では言い換えられない「ユニークな意味」を持つことを意味します。
        *   例：「量子もつれ」というノードを消した場合、近傍に「猫」や「ピザ」しかなければ、その情報を復元することは絶望的です。

    | MDL用語 | 確率的な意味 | ベクトル類似度での近似 |
    | :--- | :--- | :--- |
    | **$L(D|M)$ が低い** | 残りの情報から容易に予測・圧縮できる（冗長） | 近傍ベクトルと**高**類似度 → 「周りを見れば分かる」 |
    | **$L(D|M)$ が高い** | 他の情報から予測不可能、圧縮できない（ユニーク） | 近傍ベクトルと**低**類似度 → 「消したら誰も知らない」 |

    つまり、**「コサイン類似度が低い = 情報エントロピーが高い = 復元困難」**という等価関係が、情報理論と分散意味論の橋渡しをしているのです。

    **なぜ「厳密な確率計算」より優れているか**:
    1.  **計算効率**: 厳密なMDLでは、ノード1つ削除ごとにグラフ全体の確率分布を再計算する必要があります（$O(N^2)$以上）。ベクトル演算は、対象ノードとその近傍だけを取得すれば済むため、$O(1)$（定数時間）で完了します。
    2.  **意味の捕捉**: 言語モデルの埋め込みベクトルは、膨大なテキストデータから学習された「人間の知識体系全体」を反映しています。そのため、単純なグラフ構造だけでは分からない「意味的な冗長性」を暗黙的に評価できます。

    **結論**: ベクトル類似度は、MDLが本来計測しようとしていた「情報の復元可能性（冗長性）」を、**現代の深層学習モデルが学習した「意味空間における距離」という形で、極めて安価かつ高精度に近似するための手段**です。これにより、理論的に美しいMDLの思想を、実用的なシステムで活かすことが可能になります。

#### 4. 実行プロセス：段階的クリーンアップ
システムのパフォーマンスを維持するため、MDL判定は以下のステップで実施されます。

1.  **一次フィルタリング（高速選別）**:
    LadybugDB（Kuzu）のインデックスを利用し、「一定期間アクセスがない」「次数（エッジ数）が極めて低い」「太さが閾値以下のエッジのみを持つ」といった、削除候補ノードを高速にリストアップします。
2.  **二次判定（MDL精査）**:
    リストアップされた候補に対してのみ、前述のベクトル類似度を用いた局所MDL計算を実行します。
3.  **物理抹消（Execution）**:
    最終的なスコアが閾値を超えた（削除した方が情報効率が良い）ノードに対し、`DETACH DELETE` を実行し、データベースから物理的に削除します。

この MDL 原理に基づいた「賢い掃除」により、Cuber は単に情報を削るのではなく、**「意味の含有率」が極限まで高められ、かつ不要な贅肉が削ぎ落とされた、真に洗練された知識グラフ**を維持し続けることができるのです。

#### 5. なぜMDLが「合理的」なのか：密度の再定義
MDLを用いたアプローチが本質的である理由は、グラフの「密度」を単に「ノードあたりのエッジ数」という物理量ではなく、**「単位情報量あたりの意味の含有率」**として捉え直している点にあります。

*   **「孤立＝不要」の罠を回避**: 従来のアルゴリズムでは、つながりのないノードは無価値として一律に削除されがちでした。しかしMDLは、その情報が他で代替不可能（復元困難度が高い）であれば、たとえ孤立していても数学的に「残すべき貴重な特異点」として保護します。
*   **情報の濃縮**: MDL最小化のプロセスは、同じ意味内容をより少ない要素で表現しようとする「知識の濃縮圧」として働きます。その結果、冗長な説明は削られ、グラフ全体が「より少ない言葉（要素）で、より多くの事象を説明できる」という、知性として極めて効率的で美しい状態へと向かいます。

---

## 4. パイプラインにおける時間軸の作用メカニズム

「Absorb」「Memify」「Query」の各フェーズにおいて、これら 3 つの指標がどのように生成、洗練、そして利用されるのかを具体的に定義します。

### A. Absorb（情報の吸収：観測の瞬間）
新しいドキュメントやデータを取り込む際、`GraphExtractionTask` はその瞬間に見える関係性を抽出します。

*   **動作**: 新しく抽出されたすべてのエッジに対して、**`weight: 1.0`**, **`confidence: 1.0`**, **`unix: [現在時刻]`** を付与して保存します。
*   **設計思想**: これは「今まさに観測された情報は、システムにとって最も新鮮で、濁りのない事実である」という前提に基づいています。この「新鮮な情報への無条件の信頼」が、システムが新しい事実に即座に反応するためのエネルギーとなります。

### B. Memify（情報の定着と洗練：知識の代謝）
取り込まれた「点」としての事実は、Memify プロセス（ルールの抽出、統合、結晶化）を通じて、ネットワークの一部として組織化されます。ここが、Cuber が「成長・変化・進化」する核心部分です。

*   **成長（Reinforcement）**: 重複する関係性が繰り返し抽出される場合、`weight` や `confidence` が合算・強化され、`unix` は最新に更新されます。これにより、情報の「太さ」が増していきます。
*   **変化（Update）**: LLM が過去の矛盾を検出し、情報の修正が必要だと判断した場合、`UpdateEdgeMetrics` によって過去のエッジの値を変更します。
*   **進化（Refinement）**: 結晶化プロセスにおいて、複数の断片的な知識から一つの洗練された「ルール」が生まれる際、その背後にある古い個別の事実は、統合された新しいルールの `unix` によって包含されます。

### C. Query（情報の探索と発見：概念ドリフトの解決）
ユーザーからの問いに対して情報を検索（Graph Discovery）する際、これらの指標は「どの情報を真実として重用するか」の最終的な判断基準となります。

*   **検索時の重み付け演算**: 単なるパス探索ではなく、`weight` × `confidence` × `decay(now - unix)` のような演算を行い、**「強くて、正しくて、新しい」**情報を優先的に取得します。
*   **概念ドリフトの自動解消**: 
    1.  **古い情報の優先度低下**: 同様の意味を持つ古いエッジ（古い `unix`）は、時間的な減衰（Time Decay）によって、自然と検索結果の下位に沈んでいきます。
    2.  **矛盾発生時のディテイル解決**: 全く逆の内容を示す A と B のエッジが存在する場合、`unix` が新しい方を「現在の真実」として優先的に採用します。これにより、マニュアルでの修正なしに自動的に知識の「上書き」が行われます。
*   **成長結果の反映**: Memify を経て `weight` が高まった「太い知識」は、Query 時により確実にヒットし、システムの「コアな知識」として機能します。

---

## 4. 実装のゴール：フェーズ 27 の達成条件

開発フェーズ 27 の完遂により、Cuber は以下の能力を獲得します。

1.  **時間的永続性の獲得**: すべてのエッジが `unix` を伴い、知識の「歴史」が一意に定まる。
2.  **ドリフト耐性の向上**: 古い情報が新しい情報によって自然に淘汰（または優先度低下）されるメカニズムが、Query エンジンに組み込まれる。
3.  **生命的な知識のメタ表現**: 
    *   `weight` の増加により、知識はより「深く」なり、
    *   `confidence` の変動により、知識はより「確実」になり、
    *   `unix` の更新により、知識は常に「最新」に保たれる。

この構造的土台の上に、Query パイプラインでの「時間減衰を考慮したスコアリング」を実装することで、概念ドリフト問題に真正面から立ち向かう、真にインテリジェントな知識エンジンを実現します。
